<!doctype html><html lang=en><head><title>MiniML 0.7.0: the Grokking Update :: Simone Sturniolo's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A few months ago I released the first version of MiniML, a small machine learning framework powered by Jax. The main idea behind it was to have a very lean toolset to build models that would be as simple to train as a Scikit-learn one, while offering the same level of flexibility as PyTorch. I wrote all about it here.
A few versions later I&rsquo;ve expanded on that base with a few much needed basic modules for machine learning - radial basis function networks, multi-head self attention, a testing system that uses gold standard data from Backblaze, support for non-Scipy optimizers and more. In this release though I want to focus on the addition of two features that support research in one particular phenomenon: &ldquo;grokking&rdquo;. Let&rsquo;s see what is it!
"><meta name=keywords content=","><meta name=robots content="noodp"><link rel=canonical href=https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=https://stur86.github.io/s-plus-plus/terminal.css><link rel="shortcut icon" href=https://stur86.github.io/s-plus-plus/favicon.png><link rel=apple-touch-icon href=https://stur86.github.io/s-plus-plus/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="MiniML 0.7.0: the Grokking Update"><meta property="og:description" content="A few months ago I released the first version of MiniML, a small machine learning framework powered by Jax. The main idea behind it was to have a very lean toolset to build models that would be as simple to train as a Scikit-learn one, while offering the same level of flexibility as PyTorch. I wrote all about it here.
A few versions later I&rsquo;ve expanded on that base with a few much needed basic modules for machine learning - radial basis function networks, multi-head self attention, a testing system that uses gold standard data from Backblaze, support for non-Scipy optimizers and more. In this release though I want to focus on the addition of two features that support research in one particular phenomenon: &ldquo;grokking&rdquo;. Let&rsquo;s see what is it!
"><meta property="og:url" content="https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/"><meta property="og:site_name" content="Simone Sturniolo's Blog"><meta property="og:image" content="https://stur86.github.io/s-plus-plus/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-01-03 13:00:20 +0000 UTC"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.13.1/font/bootstrap-icons.min.css><link rel=stylesheet href=/s-plus-plus/css/custom-icons.css><link rel=stylesheet href=/s-plus-plus/css/code-styles.css></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>S++</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/s-plus-plus/>Home</a></li><li><a href=/s-plus-plus/posts>Posts</a></li><li><a href=/s-plus-plus/projects>Projects</a></li><li><a href=/s-plus-plus/about>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/s-plus-plus/>Home</a></li><li><a href=/s-plus-plus/posts>Posts</a></li><li><a href=/s-plus-plus/projects>Projects</a></li><li><a href=/s-plus-plus/about>About</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/>MiniML 0.7.0: the Grokking Update</a></h1><div class=post-meta><time class=post-date>2026-01-03</time></div><span class=post-tags>#<a href=https://stur86.github.io/s-plus-plus/tags/machine-learning/>Machine Learning</a>&nbsp;
#<a href=https://stur86.github.io/s-plus-plus/tags/ai/>AI</a>&nbsp;
#<a href=https://stur86.github.io/s-plus-plus/tags/miniml/>MiniML</a>&nbsp;
#<a href=https://stur86.github.io/s-plus-plus/tags/theory/>Theory</a>&nbsp;</span><div class=post-content><div><p>A few months ago I released the first version of <a href=https://github.com/stur86/miniml>MiniML</a>, a small machine learning framework powered by Jax. The main idea behind it was to have a very lean toolset to build models that would be as simple to train as a Scikit-learn one, while offering the same level of flexibility as PyTorch. I wrote all about it <a href=https://thedataist.substack.com/p/a-tiny-machine-learning-framework>here</a>.</p><p>A few versions later I&rsquo;ve expanded on that base with a few much needed basic modules for machine learning - radial basis function networks, multi-head self attention, a <a href=https://thedataist.substack.com/p/setting-up-data-based-tests-with>testing system that uses gold standard data from Backblaze</a>, support for non-Scipy optimizers and more. In this release though I want to focus on the addition of two features that support research in one particular phenomenon: &ldquo;grokking&rdquo;. Let&rsquo;s see what is it!</p><h2 id=the-origin-of-grokking>The origin of &ldquo;grokking&rdquo;<a href=#the-origin-of-grokking class=hanchor arialabel=Anchor>#</a></h2><p>The term &ldquo;grokking&rdquo; originates in Robert A. Heinlein&rsquo;s novel <em>Stranger in a Strange Land</em>, where it&rsquo;s used by an alien to mean achieving a depp, intimate level of understanding of something. In 2022 it was adopted in a paper by OpenAI researchers to describe a peculiar phenomenon they observed<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. The phenomenon was apparently discovered serendipitously: some researchers who were experimenting on training a small model to learn basic modular arithmetic left the training run overnight far longer than they planned. While the model had quickly learned how to reproduce its training data, it had overfit - it matched the training data very well and did very poorly on the test data. However, after being left running for a million or so of optimization steps, something else happened. All of a sudden, the loss function on the test set dropped too, and the model started performing beautifully on every data point, even those it had never seen! Beyond rote memorization, the model had &ldquo;grokked&rdquo; modular addition: it had found a way to reproduce the exact correct algorithm that produced the data with its weights, and was now able to correctly extrapolate outside of its training set. Subsequent inspection of the model&rsquo;s weights revealed a lot of regular, periodic structures - the model adopted some tricks involving sines and cosines to produce exactly the desired result of an essentially exact implementation of modular arithmetic. That&rsquo;s the gold standard for any machine learning model - to thread the line between fidelity and simplicity which eventually lands us on some true regularity, some law underlying all of the data, rather than just a lot of unrelated anecdotal observation.</p><h2 id=grokking-and-numerical-stability>Grokking and numerical stability<a href=#grokking-and-numerical-stability class=hanchor arialabel=Anchor>#</a></h2><p>Flash forward to early 2025, and a different paper came out, this time from researches of the Imperial College, London<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. This paper inspected the grokking phenomenon a bit closer and managed to achieve grokking on the same problems from the original study in as little as 300 training epochs! They did so by identifying two ways to dramatically improve the properties of the model and training process that addressed some problems that would otherwise undermine grokking. The two changes they introduced (and, spoilers, that you will find implemented in MiniML 0.7.0) are <em><strong>StableMax</strong></em> and <em><strong>OrthoGrad</strong></em>.</p><h3 id=stablemax-when-softmax-isnt-soft-enough>StableMax: when SoftMax isn&rsquo;t soft enough<a href=#stablemax-when-softmax-isnt-soft-enough class=hanchor arialabel=Anchor>#</a></h3><p>SoftMax is a common loss function used to turn an array of unbounded floats into an array of probabilities by treating them as &rsquo;logits&rsquo; of a distribution over discrete outcomes. It means that if you get an array of outputs $z_i$ for your possible bins out of a classifier, you then derive probabilities:</p><p>$$
\hat{y}_i = \text{SoftMax}(z)_i = \frac{\exp({z_i})}{\sum_j \exp({z_j})}
$$</p><p>These are used for cross-entropy loss. Since there will only be one correct class for which $y_{true} = 1$, the loss simplifies to:</p><p>$$
\mathcal{L} = -z_{true}+\log\left[\sum_j \exp({z_j})\right]
$$</p><p>The paper points out a problem with this, one which the researchers observed in their experiments. If a model converges to a good answer, one of the $z_i$ is going to be very large compared to the others. Once these are exponentiated, it can mean several orders of magnitude of difference between them. Now, when computing the gradient of the loss, the only way for gradients to &ldquo;flow through&rdquo; any outputs other than $z_{true}$ is through the logarithm of the denominator, where all the exponentials are summed. But if the differences are large enough, floating point arithmetic will make it so that these smaller terms <em>completely vanish</em>. The smallest terms are small enough, they don&rsquo;t even appear in the bigger term&rsquo;s significant digits! This effect sort of gets optimization stuck into a rut.</p><p>To avoid this effect, the researchers propose replacing SoftMax with a slower changing function with fatter tails, which they dub StableMax. First they define a function $s(x)$ as follows:</p><p>$$
s(x) = \begin{cases}
1+x & x > 0 \\<br>\frac{1}{1-x} & x \le 0<br>\end{cases}
$$</p><p>Then they define StableMax as:</p><p>$$
\text{StableMax}(z)_i = \frac{s(z_i)}{\sum_j s(z_j)}
$$</p><p>You can see here how the two functions compare (on the left a comparison of exponential vs $s(x)$, on the right a comparison of the resulting sigmoid probabilities for a varying $x$ referenced against zero):</p><p><img src=soft_vs_stable_max.png alt="Comparison between SoftMax and StableMax"></p><p>The slower decay in StableMax allows the loss gradient to be more sensitive even to weights that participate more in the low-probability bins.</p><h3 id=orthograd-fixing-the-root-of-the-issue>OrthoGrad: fixing the root of the issue<a href=#orthograd-fixing-the-root-of-the-issue class=hanchor arialabel=Anchor>#</a></h3><p>While StableMax can improve the numerical stability problem, it does not fix the root cause of the issue. That cause can be understood simply if you consider this toy example:</p><p>imagine you have logistic regressor. A simple linear layer multiplied by your input vector, whose outputs are treated as logits for the $N$ classes of your problem. You&rsquo;ve reached a stage in training where the results are already good enough (if you take the highest logits you have accuracy of 100% on your training set). How can the loss improve any further? One easy answer is: by scaling upwards every weight in the layer! If the weights are scaled up, the logits are also scaled. The positive ones will become more positive, and the negative more negative; the model will become more &ldquo;certain&rdquo; of the result and thus the loss will go down.</p><p>This does not improve the <em>actual</em> predictive power of the model one bit. In fact, this actively worsens its ability to generalize and overfits more and more the specific pattern that was found in the training data. Normally, this trend is contained by regularization, but when trying to achieve &ldquo;grokking&rdquo; it&rsquo;s actually usually better to apply no regularization at all - to avoid damping the model&rsquo;s ability to converge to the correct algorithm, and given the fact that we know that our training data is entirely noiseless.</p><p>If you consider the weights of the model as a vector $\theta$, then this kind of scaling means performing a step that&rsquo;s essentially parallel to the original vector. In practice, the gradient of the loss, $g = \nabla_\theta \mathcal{L}$, is going to have a parallel and a perpendicular component to $\theta$, but the parallel component is going to be much more important if that&rsquo;s an easy way for the model to reduce its loss. The OrthoGrad approach means simply that the update step is not computed any more using the full gradient $g$, but instead, <em>using only the perpendicular component to the weight vector</em>, $g_\perp$. This way, the updates keep exploring the space of possible solutions in more interesting directions rather than doubling down on the one that was already found. The paper uses AdamW but in theory this approach should be usable with some other optimizers too - not the ones relying on Hessian approximations though, I expect, unless the Hessian is similarly adapted.</p><h2 id=grokking-in-miniml>Grokking in MiniML<a href=#grokking-in-miniml class=hanchor arialabel=Anchor>#</a></h2><p>In MiniML 0.7.0 I implemented two things to support &ldquo;grokking&rdquo; experiments:</p><ol><li>I&rsquo;ve added several utility functions to support StableMax: in particular <a href=https://stur86.github.io/miniml/api/miniml/loss/#miniml.loss.stablemax><code>stablemax</code></a>, <a href=https://stur86.github.io/miniml/api/miniml/loss/#miniml.loss.log_stablemax><code>log_stablemax</code></a> and <a href=https://stur86.github.io/miniml/api/miniml/loss/#miniml.loss.CrossEntropyStableMaxLogLoss><code>CrossEntropyStableMaxLogLoss</code></a>;</li><li>I&rsquo;ve added an <code>ortho_grad</code> option to the <a href=https://stur86.github.io/miniml/api/miniml/optim/adam/>Adam optimizers</a>, which enables orthogonalized gradients for it; the same option also exists in <a href=https://stur86.github.io/miniml/api/miniml/optim/base/#miniml.optim.base.OptimizationMethods.Config><code>OptimizationMethods.Config</code></a> so that it can be easily used in future optimizers as well;</li><li>I&rsquo;ve added a <a href=https://github.com/stur86/miniml/blob/main/examples/example_6_grokking.py>new example Marimo notebook</a> that puts these to use in the same test as the 2025 paper.</li></ol><p>You can see the results of the notebook here:</p><p><img src=grokking_miniml.png alt="Grokking with MiniML"></p><p>early on, there&rsquo;s a very fast drop in the train loss while the test loss remains high. Later, the test loss has itself a drop, albeit not as dramatic; to this drop corresponds a jump in accuracy that hits 100%, with perfect generalization of the algorithm outside of the training domain!</p><p>Find MiniML <a href=https://github.com/stur86/miniml>here on Github</a>; <a href=https://stur86.github.io/miniml/>here</a> are its docs, and <a href=https://pypi.org/project/miniml-jax/#description>here is its PyPI page</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, <em>Grokking: generalization beyond overfitting on small algorithmic datasets</em>, <a href=https://arxiv.org/abs/2201.02177>arXiv</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Lucas Prieto, Melih Barsbey, Pedro A.M. Mediano, Tolga Birdal, <em>Grokking at the edge of numerical stability</em>, <a href=https://arxiv.org/abs/2501.04697>arXiv</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://stur86.github.io/s-plus-plus/posts/the-big-learning-set-for-big-world-helpers/ class="button inline next">[<span class=button__text>The Big Learning Set for Big World Helpers</span>] ></a></div></div><script src=https://giscus.app/client.js data-repo=stur86/blog-giscus-comments data-repo-id=R_kgDOQawHpg data-category=Announcements data-category-id=DIC_kwDOQawHps4CyEyI data-mapping=og:title data-reactions-enabled=0 data-emit-metadata=0 data-input-position=bottom data-theme=dark data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/s-plus-plus/bundle.min.js></script></div></body></html>