<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on Simone Sturniolo's Blog</title><link>https://stur86.github.io/s-plus-plus/tags/ai/</link><description>Recent content in AI on Simone Sturniolo's Blog</description><generator>Hugo</generator><language>en-uk</language><lastBuildDate>Sat, 03 Jan 2026 13:00:20 +0000</lastBuildDate><atom:link href="https://stur86.github.io/s-plus-plus/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>MiniML 0.7.0: the Grokking Update</title><link>https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/</link><pubDate>Sat, 03 Jan 2026 13:00:20 +0000</pubDate><guid>https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/</guid><description>&lt;p&gt;A few months ago I released the first version of &lt;a href="https://github.com/stur86/miniml"&gt;MiniML&lt;/a&gt;, a small machine learning framework powered by Jax. The main idea behind it was to have a very lean toolset to build models that would be as simple to train as a Scikit-learn one, while offering the same level of flexibility as PyTorch. I wrote all about it &lt;a href="https://thedataist.substack.com/p/a-tiny-machine-learning-framework"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A few versions later I&amp;rsquo;ve expanded on that base with a few much needed basic modules for machine learning - radial basis function networks, multi-head self attention, a &lt;a href="https://thedataist.substack.com/p/setting-up-data-based-tests-with"&gt;testing system that uses gold standard data from Backblaze&lt;/a&gt;, support for non-Scipy optimizers and more. In this release though I want to focus on the addition of two features that support research in one particular phenomenon: &amp;ldquo;grokking&amp;rdquo;. Let&amp;rsquo;s see what is it!&lt;/p&gt;</description></item><item><title>The Big Learning Set for Big World Helpers</title><link>https://stur86.github.io/s-plus-plus/posts/the-big-learning-set-for-big-world-helpers/</link><pubDate>Thu, 04 Dec 2025 07:23:00 +0000</pubDate><guid>https://stur86.github.io/s-plus-plus/posts/the-big-learning-set-for-big-world-helpers/</guid><description>&lt;p&gt;On November 12, 2012, Randall Munroe&amp;rsquo;s famous xkcd comic published &lt;a href="https://xkcd.com/1133/"&gt;Up Goer Five&lt;/a&gt;, a blueprint and explanation of the Apollo V rocket written using only the 1000 most common words of the English language (as he estimated them). Later on, on November 24, 2015, came out &lt;a href="https://xkcd.com/thing-explainer/"&gt;Thing Explainer&lt;/a&gt;, an entire illustrated book of similar explanations for other objects and concepts. The &amp;ldquo;only the most common 1000 words&amp;rdquo; style of writing sounds sometimes stilted, sometimes a bit funny, but these texts certainly prove that it&amp;rsquo;s &lt;em&gt;enough&lt;/em&gt; to talk virtually about anything.&lt;/p&gt;
&lt;p&gt;In the age of LLMs, would it be possible to have a training set built only on the most common 1000 words of the English language?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try.&lt;/p&gt;</description></item></channel></rss>