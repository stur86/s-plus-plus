<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MiniML on Simone Sturniolo's Blog</title><link>https://stur86.github.io/s-plus-plus/tags/miniml/</link><description>Recent content in MiniML on Simone Sturniolo's Blog</description><generator>Hugo</generator><language>en-uk</language><lastBuildDate>Sat, 03 Jan 2026 13:00:20 +0000</lastBuildDate><atom:link href="https://stur86.github.io/s-plus-plus/tags/miniml/index.xml" rel="self" type="application/rss+xml"/><item><title>MiniML 0.7.0: the Grokking Update</title><link>https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/</link><pubDate>Sat, 03 Jan 2026 13:00:20 +0000</pubDate><guid>https://stur86.github.io/s-plus-plus/posts/miniml-the-grokking-update/</guid><description>&lt;p&gt;A few months ago I released the first version of &lt;a href="https://github.com/stur86/miniml"&gt;MiniML&lt;/a&gt;, a small machine learning framework powered by Jax. The main idea behind it was to have a very lean toolset to build models that would be as simple to train as a Scikit-learn one, while offering the same level of flexibility as PyTorch. I wrote all about it &lt;a href="https://thedataist.substack.com/p/a-tiny-machine-learning-framework"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A few versions later I&amp;rsquo;ve expanded on that base with a few much needed basic modules for machine learning - radial basis function networks, multi-head self attention, a &lt;a href="https://thedataist.substack.com/p/setting-up-data-based-tests-with"&gt;testing system that uses gold standard data from Backblaze&lt;/a&gt;, support for non-Scipy optimizers and more. In this release though I want to focus on the addition of two features that support research in one particular phenomenon: &amp;ldquo;grokking&amp;rdquo;. Let&amp;rsquo;s see what is it!&lt;/p&gt;</description></item></channel></rss>